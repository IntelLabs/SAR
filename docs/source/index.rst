.. SAR documentation master file, created by
   sphinx-quickstart on Mon Mar 28 07:30:12 2022.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.


Welcome to SAR's documentation!
===============================

   
SAR is a pure Python library built on top of  `DGL <https://www.dgl.ai/>`_ to accelerate distributed training of Graph Neural Networks (GNNs) on large graphs. SAR supports both full-batch training and sampling-based training. For full-batch training, SAR supports the `Sequenial Aggregation and Rematerialization (SAR)  <https://proceedings.mlsys.org/paper/2022/hash/5fd0b37cd7dbbb00f97ba6ce92bf5add-Abstract.html>`_  scheme to reduce peak per-machine memory consumption and guarantee that model memory consumption per worker goes down linearly with the number of workers.  This is achieved by eliminating most of the data redundancy (due to the halo effect) involved in standard spatially parallel training. 

SAR uses the graph partition data generated by DGL's `partitioning utilities <https://docs.dgl.ai/en/0.6.x/generated/dgl.distributed.partition.partition_graph.html>`_. It can thus be used as a drop in replacement for DGL's sampling-based distributed training. SAR enables scalable, distributed training on very large graphs, and supports multiple training modes that balance speed against memory efficiency. SAR requires minimal changes  to existing single-host DGL training code. See the quick start guide to get started using SAR.

.. toctree::
   :maxdepth: 1

   Quick start<quick_start>
   Data loading and graph construction <data_loading>
   Communication routines<comm>
   Full-batch training<full_batch>
   Sampling-based training<sampling_training>
   Data tuples<common_tuples>
   SAR Configuration<sar_config.rst>



Index
==================

* :ref:`genindex`

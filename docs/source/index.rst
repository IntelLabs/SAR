.. SAR documentation master file, created by
   sphinx-quickstart on Mon Mar 28 07:30:12 2022.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.


Welcome to SAR's documentation!
===============================

   
Sequential Aggregation and Rematerialization (SAR) is a pure Python library built on top of  `DGL <https://www.dgl.ai/>`_ to accelerate distributed full-batch training of Graph Neural Networks (GNNs) on large graphs. The core innovation in SAR is the sequential rematerialization scheme that SAR employs in the backward pass to keep memory consumption in check. SAR guarantees that the memory required to store node features in each worker will go down linearly with the number of workers, even for densely connected graphs. This is achieved by eliminating most of the data redundancy (due to the halo effect) involved in standard spatially parallel training. 

SAR uses the graph partition data generated by DGL's `partitioning utilities <https://docs.dgl.ai/en/0.6.x/generated/dgl.distributed.partition.partition_graph.html>`_. It can thus be used as a drop in replacement for DGL's sampling-based distributed training. SAR enables scalable, distributed training on very large graphs, and supports multiple training modes that balance speed against memory efficiency. SAR requires minimal changes  to existing single-host DGL training code. See the quick start guide to get started using SAR.

.. toctree::
   :maxdepth: 1

   Quick start<quick_start>
   Data loading and graph construction <data_loading>
   Communication routines<comm>
   Preparing your GNN for SAR<model_prepare>
   Training modes <sar_modes>
   Distributed Graph Objects<shards>
   Data tuples<common_tuples>
   SAR Configuration<sar_config.rst>



Index
==================

* :ref:`genindex`
